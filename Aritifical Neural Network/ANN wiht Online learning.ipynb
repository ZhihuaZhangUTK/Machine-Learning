{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('spambase.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get the dummpy variable of labels\n",
    "df = pd.get_dummies(df, columns=[57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check the missing values, there are no missing values in every variable.\n",
    "sum(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_x  = df.iloc[:,:-2]\n",
    "df_y = df.iloc[:, [-1,-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.342396</td>\n",
       "      <td>0.330849</td>\n",
       "      <td>0.712781</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>-0.350228</td>\n",
       "      <td>-0.291762</td>\n",
       "      <td>-0.262533</td>\n",
       "      <td>-0.323267</td>\n",
       "      <td>-0.371324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.514251</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>0.623939</td>\n",
       "      <td>-0.308321</td>\n",
       "      <td>-0.103037</td>\n",
       "      <td>-0.045242</td>\n",
       "      <td>0.045293</td>\n",
       "      <td>-0.008723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345322</td>\n",
       "      <td>0.051904</td>\n",
       "      <td>0.435082</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>-0.256089</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.244717</td>\n",
       "      <td>-0.088001</td>\n",
       "      <td>-0.323267</td>\n",
       "      <td>1.086593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.026004</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>0.126189</td>\n",
       "      <td>0.423737</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.250536</td>\n",
       "      <td>1.228191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.145906</td>\n",
       "      <td>-0.165054</td>\n",
       "      <td>0.851631</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>1.364698</td>\n",
       "      <td>0.343648</td>\n",
       "      <td>0.193623</td>\n",
       "      <td>0.036666</td>\n",
       "      <td>1.973802</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.117364</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.440005</td>\n",
       "      <td>-0.079746</td>\n",
       "      <td>0.145905</td>\n",
       "      <td>2.220865</td>\n",
       "      <td>3.258378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.342396</td>\n",
       "      <td>-0.165054</td>\n",
       "      <td>-0.556700</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>0.472521</td>\n",
       "      <td>-0.350228</td>\n",
       "      <td>0.500183</td>\n",
       "      <td>1.308259</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.605791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.007510</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>-0.161917</td>\n",
       "      <td>-0.308321</td>\n",
       "      <td>-0.103037</td>\n",
       "      <td>-0.052144</td>\n",
       "      <td>-0.062459</td>\n",
       "      <td>-0.152205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.342396</td>\n",
       "      <td>-0.165054</td>\n",
       "      <td>-0.556700</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>0.472521</td>\n",
       "      <td>-0.350228</td>\n",
       "      <td>0.500183</td>\n",
       "      <td>1.308259</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.605791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.014908</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>-0.164369</td>\n",
       "      <td>-0.308321</td>\n",
       "      <td>-0.103037</td>\n",
       "      <td>-0.052144</td>\n",
       "      <td>-0.062459</td>\n",
       "      <td>-0.152205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.342396  0.330849  0.712781 -0.046894  0.011563 -0.350228 -0.291762   \n",
       "1  0.345322  0.051904  0.435082 -0.046894 -0.256089  0.672326  0.244717   \n",
       "2 -0.145906 -0.165054  0.851631 -0.046894  1.364698  0.343648  0.193623   \n",
       "3 -0.342396 -0.165054 -0.556700 -0.046894  0.472521 -0.350228  0.500183   \n",
       "4 -0.342396 -0.165054 -0.556700 -0.046894  0.472521 -0.350228  0.500183   \n",
       "\n",
       "         7         8         9     ...           47        48        49  \\\n",
       "0 -0.262533 -0.323267 -0.371324    ...    -0.111534 -0.158436 -0.514251   \n",
       "1 -0.088001 -0.323267  1.086593    ...    -0.111534 -0.158436 -0.026004   \n",
       "2  0.036666  1.973802  0.016420    ...    -0.111534 -0.117364  0.014683   \n",
       "3  1.308259  0.789376  0.605791    ...    -0.111534 -0.158436 -0.007510   \n",
       "4  1.308259  0.789376  0.605791    ...    -0.111534 -0.158436 -0.014908   \n",
       "\n",
       "         50        51        52        53        54        55        56  \n",
       "0 -0.155181  0.623939 -0.308321 -0.103037 -0.045242  0.045293 -0.008723  \n",
       "1 -0.155181  0.126189  0.423737  0.008762 -0.002443  0.250536  1.228191  \n",
       "2 -0.155181  0.008495  0.440005 -0.079746  0.145905  2.220865  3.258378  \n",
       "3 -0.155181 -0.161917 -0.308321 -0.103037 -0.052144 -0.062459 -0.152205  \n",
       "4 -0.155181 -0.164369 -0.308321 -0.103037 -0.052144 -0.062459 -0.152205  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardlize data\n",
    "df_x_std = (df_x-df_x.mean()) / df_x.std()\n",
    "df_x_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN implementation with online learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes):\n",
    "        # parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.num_layer = num_layer\n",
    "        self.num_nodes = num_nodes\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # initialize the weight matrix \n",
    "        self.W = [np.random.randn(self.input_dim, self.num_nodes)]  # input to hidden layer\n",
    "        self.B = [np.zeros((1, self.num_nodes))]\n",
    "        for _ in range(1, self.num_layer):\n",
    "            self.W.append(np.random.randn(self.num_nodes,self.num_nodes))   # hidden layer to hidden layer\n",
    "            self.B.append(np.zeros((1, self.num_nodes)))\n",
    "        self.W.append(np.random.randn(self.num_nodes,self.output_dim))  # hiddern layer to output \n",
    "        self.B.append(np.zeros((1, self.output_dim)))      \n",
    "    \n",
    "    # activate method: sigmoid\n",
    "    def sigmoid(self, s):\n",
    "        return 1.0/(1 + np.exp(-s))\n",
    "    def sigmod_prime(self, s):\n",
    "        return s * (1-s)\n",
    "    \n",
    "    ## forward\n",
    "    def forward(self, row):\n",
    "        # input layer to output layer forward\n",
    "        self.Z = [np.array(row).reshape(1, self.input_dim)]\n",
    "        for i in range(0, len(self.W)):\n",
    "            z = np.dot(self.Z[i], self.W[i]) + self.B[i]\n",
    "            sz = self.sigmoid(z)\n",
    "            self.Z.append(sz)\n",
    "        # return output\n",
    "        return self.Z[-1]\n",
    "    \n",
    "    # backward\n",
    "    def backward(self,Y, output):\n",
    "        self.deltaZ = [(output-Y) * self.sigmod_prime(output)]\n",
    "        self.deltaW = []\n",
    "        self.deltaB = []\n",
    "        for i in range(len(self.W)-1,-1,-1):\n",
    "            deltaW = np.dot(self.Z[i].T, self.deltaZ[-1])\n",
    "            self.deltaW = [deltaW] + self.deltaW\n",
    "            deltaB = np.sum(self.deltaZ[-1], axis=0)\n",
    "            self.deltaB = [deltaB] + self.deltaB\n",
    "            \n",
    "            # add backward deltaZ in self.Z\n",
    "            dz = np.multiply((self.deltaZ[-1] @ self.W[i].T), self.sigmod_prime(self.Z[i]))\n",
    "            self.deltaZ.append(dz)\n",
    "        \n",
    "        # update the parameters\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] += -1*self.learning_rate*self.deltaW[i]\n",
    "            self.B[i] += -1*self.learning_rate*self.deltaB[i]\n",
    "    \n",
    "    # train the data \n",
    "    def train(self, train_x, train_y, print_loss=False):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "\n",
    "        for n in range(self.max_iter):         \n",
    "            #### online training \n",
    "            for i in range(len(self.train_x)):\n",
    "                output  = np.array(self.forward(train_x[i]))\n",
    "                self.backward(np.array(train_y[i]), output)\n",
    "                \n",
    "            # Forward propogation to calculate the predictions\n",
    "            if print_loss:\n",
    "                losses = []\n",
    "                for i in range(len(train_x)):\n",
    "                    loss =- np.sum(train_y[i] * np.log(self.forward(train_x[i])))\n",
    "                    losses.append(loss)\n",
    "                print(\"Loss after iteration %i: %f\" %(i, np.sum(losses) / len(train_x)))\n",
    "                \n",
    "    def predict(self, val_x):\n",
    "        # Do forward pass\n",
    "        predictions = []\n",
    "        for i in range(len(val_x)):\n",
    "            pred = self.forward(val_x[i])\n",
    "            #get y_hat\n",
    "            y_hat = np.argmax(pred, axis=1)\n",
    "            predictions.append(y_hat[0])\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def accuracy_score(self,pred,y):\n",
    "        # Get total number of examples\n",
    "        m = y.shape[0]\n",
    "        y_true = y.argmax(axis=1)\n",
    "        # Calculate the number of wrong examples\n",
    "        error = np.sum(np.abs(pred-y_true)) \n",
    "        # Calculate accuracy\n",
    "        return (m-error) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## random split data, training dataset (80%) and test dataset (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "## data\n",
    "X = np.array(df_x_std)\n",
    "Y = np.array(df_y)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## run k-folder cross validation function \n",
    "def cv_train_test_split(X, Y, kfolder= 10):\n",
    "    X_split, Y_split = [], []\n",
    "    index = list(range(len(Y)))\n",
    "    random.shuffle(index)   ## shuffle the index to random select\n",
    "    fold_size = int(len(X) / kfolder) + (len(X)%kfolder > 0)\n",
    "    for i in range(kfolder):\n",
    "        X_fold = X[index[i*fold_size: (i+1)*fold_size]]\n",
    "        Y_fold = Y[index[i*fold_size: (i+1)*fold_size]]\n",
    "        X_split.append(X_fold)\n",
    "        Y_split.append(Y_fold)\n",
    "    return X_split, Y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data split\n",
    "nn_X, nn_Y = cv_train_test_split(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## run the cross validation function \n",
    "def cv_nn(X, Y, input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes):\n",
    "    scores = []\n",
    "    for i in range(10):\n",
    "        x_train, x_test = np.concatenate(X[:i] + X[i+1:], axis = 0), X[i]\n",
    "        y_train, y_test = np.concatenate(Y[:i] + Y[i+1:], axis = 0), Y[i]\n",
    "        NN = Neural_Network(input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "        NN.train(x_train, y_train)\n",
    "        pred = NN.predict(x_test)\n",
    "        scores.append(NN.accuracy_score(pred, y_test))\n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for 0.001000, 5 is 0.5878\n",
      "The accuracy score for 0.001000, 10 is 0.6538\n",
      "The accuracy score for 0.001000, 15 is 0.7212\n",
      "The accuracy score for 0.001000, 20 is 0.7389\n",
      "The accuracy score for 0.001000, 25 is 0.7462\n",
      "The accuracy score for 0.001000, 30 is 0.7658\n",
      "The accuracy score for 0.001000, 35 is 0.8071\n",
      "The accuracy score for 0.001000, 40 is 0.7970\n",
      "The accuracy score for 0.001000, 45 is 0.8095\n",
      "The accuracy score for 0.001000, 50 is 0.8378\n",
      "The accuracy score for 0.005000, 5 is 0.7454\n",
      "The accuracy score for 0.005000, 10 is 0.8217\n",
      "The accuracy score for 0.005000, 15 is 0.8633\n",
      "The accuracy score for 0.005000, 20 is 0.8834\n",
      "The accuracy score for 0.005000, 25 is 0.8970\n",
      "The accuracy score for 0.005000, 30 is 0.9024\n",
      "The accuracy score for 0.005000, 35 is 0.8978\n",
      "The accuracy score for 0.005000, 40 is 0.9125\n",
      "The accuracy score for 0.005000, 45 is 0.9130\n",
      "The accuracy score for 0.005000, 50 is 0.9139\n",
      "The accuracy score for 0.010000, 5 is 0.8177\n",
      "The accuracy score for 0.010000, 10 is 0.8788\n",
      "The accuracy score for 0.010000, 15 is 0.9027\n",
      "The accuracy score for 0.010000, 20 is 0.9136\n",
      "The accuracy score for 0.010000, 25 is 0.9179\n",
      "The accuracy score for 0.010000, 30 is 0.9204\n",
      "The accuracy score for 0.010000, 35 is 0.9245\n",
      "The accuracy score for 0.010000, 40 is 0.9280\n",
      "The accuracy score for 0.010000, 45 is 0.9247\n",
      "The accuracy score for 0.010000, 50 is 0.9264\n",
      "The accuracy score for 0.050000, 5 is 0.9160\n",
      "The accuracy score for 0.050000, 10 is 0.9266\n",
      "The accuracy score for 0.050000, 15 is 0.9296\n",
      "The accuracy score for 0.050000, 20 is 0.9266\n",
      "The accuracy score for 0.050000, 25 is 0.9302\n",
      "The accuracy score for 0.050000, 30 is 0.9315\n",
      "The accuracy score for 0.050000, 35 is 0.9313\n",
      "The accuracy score for 0.050000, 40 is 0.9359\n",
      "The accuracy score for 0.050000, 45 is 0.9323\n",
      "The accuracy score for 0.050000, 50 is 0.9359\n",
      "The accuracy score for 0.100000, 5 is 0.9274\n",
      "The accuracy score for 0.100000, 10 is 0.9299\n",
      "The accuracy score for 0.100000, 15 is 0.9367\n",
      "The accuracy score for 0.100000, 20 is 0.9321\n",
      "The accuracy score for 0.100000, 25 is 0.9348\n",
      "The accuracy score for 0.100000, 30 is 0.9337\n",
      "The accuracy score for 0.100000, 35 is 0.9340\n",
      "The accuracy score for 0.100000, 40 is 0.9329\n",
      "The accuracy score for 0.100000, 45 is 0.9329\n",
      "The accuracy score for 0.100000, 50 is 0.9321\n",
      "The accuracy score for 0.500000, 5 is 0.9304\n",
      "The accuracy score for 0.500000, 10 is 0.9245\n",
      "The accuracy score for 0.500000, 15 is 0.9274\n",
      "The accuracy score for 0.500000, 20 is 0.9332\n",
      "The accuracy score for 0.500000, 25 is 0.9296\n",
      "The accuracy score for 0.500000, 30 is 0.9315\n",
      "The accuracy score for 0.500000, 35 is 0.9307\n",
      "The accuracy score for 0.500000, 40 is 0.9266\n",
      "The accuracy score for 0.500000, 45 is 0.9356\n",
      "The accuracy score for 0.500000, 50 is 0.9315\n",
      "The accuracy score for 1.000000, 5 is 0.9266\n",
      "The accuracy score for 1.000000, 10 is 0.9342\n",
      "The accuracy score for 1.000000, 15 is 0.9231\n",
      "The accuracy score for 1.000000, 20 is 0.9293\n",
      "The accuracy score for 1.000000, 25 is 0.9253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for 1.000000, 30 is 0.9236\n",
      "The accuracy score for 1.000000, 35 is 0.9296\n",
      "The accuracy score for 1.000000, 40 is 0.9247\n",
      "The accuracy score for 1.000000, 45 is 0.9332\n",
      "The accuracy score for 1.000000, 50 is 0.9288\n"
     ]
    }
   ],
   "source": [
    "## tune the parameters: 1. max_iter   2. learning rate\n",
    "# specified parameters, to simplify we choose to have one hidden layer with 5 neurons.\n",
    "input_dim = train_x.shape[1]\n",
    "output_dim = 2\n",
    "num_layer = 1\n",
    "num_nodes = 5\n",
    "\n",
    "\n",
    "# unspecifies parameters\n",
    "learning_rates = [ 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_iters = list(range(5, 55,5))\n",
    "score_matrix = np.zeros((len(learning_rates), len(max_iters)))\n",
    "\n",
    "for l in range(len(learning_rates)):\n",
    "    for m in range(len(max_iters)):\n",
    "        learning_rate, max_iter = learning_rates[l], max_iters[m]\n",
    "        accu = cv_nn(nn_X, nn_Y, input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "        print('The accuracy score for %f, %d is %.4f' %(learning_rate, max_iter, accu))\n",
    "        score_matrix[l, m] = accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## heatmap\n",
    "def plotHeatMap(data, x_title='X Axis', y_title='Y Axis', title='', x_ticks=[], y_ticks=[]):\n",
    "    \n",
    "    #Plot it out\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    heatmap = ax.pcolor(data, edgecolors='k', alpha=0.8)\n",
    "    \n",
    "    fig.colorbar(heatmap, ax=ax)\n",
    "    \n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(len(x_ticks)) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(len(y_ticks)) + 0.5, minor=False)\n",
    "    \n",
    "    ax.set_xticklabels([str(i) for i in x_ticks], minor=False)\n",
    "    ax.set_yticklabels([str(y) for y in y_ticks], minor=False)\n",
    "    \n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bfdaba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotHeatMap(score_matrix, y_title='learning rate', x_title='max iter',\\\n",
    "            title='Accuracy Score', y_ticks=learning_rates, x_ticks=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pred, y):\n",
    "    m = y.shape[0]\n",
    "    y_true = y.argmax(axis=1)\n",
    "    \n",
    "    FP = sum((y_true==0) & (pred==1))\n",
    "    FN = sum((y_true==1) & (pred==0))\n",
    "    TP = sum((y_true==1) & (pred==1))\n",
    "    TN = sum((y_true==0) & (pred==0))\n",
    "    \n",
    "    ## print performance metrics\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    TPR = TP / (TP + FN)\n",
    "    PPV = TP / (TP + FP)\n",
    "    TNR = TN / (TN + FP)\n",
    "    F1_score = 2*PPV*TPR / (PPV+TPR)\n",
    "    metric = pd.DataFrame([accuracy, TPR, PPV, TNR, F1_score], ['ACCU', 'TPR', 'PPV', 'TNR', 'F1'], ['score'])\n",
    "    \n",
    "    ### print confusion matrix\n",
    "    conf_matrix = np.array([[TP, FN], [FP, TN]])\n",
    "    conf = pd.DataFrame(conf_matrix, ['spam', 'not spam'], ['spam', 'not spam'])\n",
    "    return metric, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 3679: 0.337720\n",
      "Loss after iteration 3679: 0.256780\n",
      "Loss after iteration 3679: 0.230917\n",
      "Loss after iteration 3679: 0.214420\n",
      "Loss after iteration 3679: 0.205818\n",
      "Loss after iteration 3679: 0.199897\n",
      "Loss after iteration 3679: 0.196530\n",
      "Loss after iteration 3679: 0.191665\n",
      "Loss after iteration 3679: 0.188837\n",
      "Loss after iteration 3679: 0.186674\n",
      "Loss after iteration 3679: 0.184102\n",
      "Loss after iteration 3679: 0.182587\n",
      "Loss after iteration 3679: 0.181700\n",
      "Loss after iteration 3679: 0.180042\n",
      "Loss after iteration 3679: 0.180500\n",
      "         score\n",
      "ACCU  0.927253\n",
      "TPR   0.925795\n",
      "PPV   0.954463\n",
      "TNR   0.929577\n",
      "F1    0.939910\n",
      "          spam  not spam\n",
      "spam       524        42\n",
      "not spam    25       330\n"
     ]
    }
   ],
   "source": [
    "## find the optimum learning_rate and max_iter, train the training data and test the test_data\n",
    "learning_rate, max_iter = 0.1, 15\n",
    "NN = Neural_Network(input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "NN.train(train_x, train_y, print_loss=True)\n",
    "pred = NN.predict(test_x)\n",
    "# metrics \n",
    "metric, confusion = confusion_matrix(pred, test_y)\n",
    "print(metric)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNs with Reduced Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11564794 0.17297118 0.2081142  0.23641524 0.26354169 0.28920029\n",
      " 0.314007   0.33812839 0.36085269 0.38325521 0.4046048  0.42443115\n",
      " 0.4439361  0.46314837 0.48221912 0.50087647 0.51927371 0.53722774\n",
      " 0.55499377 0.57258701 0.59005918 0.60722171 0.62414334 0.64065816\n",
      " 0.65708953 0.67330598 0.68936487 0.70523575 0.72055892 0.73575189\n",
      " 0.75042723 0.7649313  0.77892553 0.79263965 0.80626455 0.81951942\n",
      " 0.83239801 0.84508326 0.85744449 0.86954722 0.88138239 0.8930665\n",
      " 0.90392819 0.91460277 0.92481097 0.93493523 0.94413616 0.95271015\n",
      " 0.96061065 0.96778785 0.97437741 0.98079342 0.98666807 0.99202564\n",
      " 0.99659442 0.99993237 1.        ]\n"
     ]
    }
   ],
   "source": [
    "## run pca analysis, get the principle components, the first 43 components (90% of variance)\n",
    "u, s, vh = np.linalg.svd(df_x_std)\n",
    "\n",
    "s2 = np.cumsum(s**2) / np.sum(s**2)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_x = df_x_std @ vh.T[:, :43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## data\n",
    "X_PC = np.array(pc_x)\n",
    "Y_PC = np.array(df_y)\n",
    "\n",
    "pca_train_x, pca_test_x, pca_train_y, pca_test_y = train_test_split(X_PC, Y_PC, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data split\n",
    "nn_pca_X, nn_pca_Y = cv_train_test_split(pca_train_x, pca_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for 0.001000, 5 is 0.6141\n",
      "The accuracy score for 0.001000, 10 is 0.6970\n",
      "The accuracy score for 0.001000, 15 is 0.6796\n",
      "The accuracy score for 0.001000, 20 is 0.7429\n",
      "The accuracy score for 0.001000, 25 is 0.7717\n",
      "The accuracy score for 0.001000, 30 is 0.7793\n",
      "The accuracy score for 0.001000, 35 is 0.8027\n",
      "The accuracy score for 0.001000, 40 is 0.8394\n",
      "The accuracy score for 0.001000, 45 is 0.8296\n",
      "The accuracy score for 0.001000, 50 is 0.8378\n",
      "The accuracy score for 0.005000, 5 is 0.7772\n",
      "The accuracy score for 0.005000, 10 is 0.8492\n",
      "The accuracy score for 0.005000, 15 is 0.8810\n",
      "The accuracy score for 0.005000, 20 is 0.8970\n",
      "The accuracy score for 0.005000, 25 is 0.8997\n",
      "The accuracy score for 0.005000, 30 is 0.9043\n",
      "The accuracy score for 0.005000, 35 is 0.9158\n",
      "The accuracy score for 0.005000, 40 is 0.9128\n",
      "The accuracy score for 0.005000, 45 is 0.9212\n",
      "The accuracy score for 0.005000, 50 is 0.9158\n",
      "The accuracy score for 0.010000, 5 is 0.8470\n",
      "The accuracy score for 0.010000, 10 is 0.8853\n",
      "The accuracy score for 0.010000, 15 is 0.9046\n",
      "The accuracy score for 0.010000, 20 is 0.9139\n",
      "The accuracy score for 0.010000, 25 is 0.9236\n",
      "The accuracy score for 0.010000, 30 is 0.9236\n",
      "The accuracy score for 0.010000, 35 is 0.9255\n",
      "The accuracy score for 0.010000, 40 is 0.9239\n",
      "The accuracy score for 0.010000, 45 is 0.9220\n",
      "The accuracy score for 0.010000, 50 is 0.9255\n",
      "The accuracy score for 0.050000, 5 is 0.9204\n",
      "The accuracy score for 0.050000, 10 is 0.9258\n",
      "The accuracy score for 0.050000, 15 is 0.9239\n",
      "The accuracy score for 0.050000, 20 is 0.9283\n",
      "The accuracy score for 0.050000, 25 is 0.9274\n",
      "The accuracy score for 0.050000, 30 is 0.9285\n",
      "The accuracy score for 0.050000, 35 is 0.9293\n",
      "The accuracy score for 0.050000, 40 is 0.9296\n",
      "The accuracy score for 0.050000, 45 is 0.9280\n",
      "The accuracy score for 0.050000, 50 is 0.9269\n",
      "The accuracy score for 0.100000, 5 is 0.9223\n",
      "The accuracy score for 0.100000, 10 is 0.9291\n",
      "The accuracy score for 0.100000, 15 is 0.9291\n",
      "The accuracy score for 0.100000, 20 is 0.9304\n",
      "The accuracy score for 0.100000, 25 is 0.9255\n",
      "The accuracy score for 0.100000, 30 is 0.9299\n",
      "The accuracy score for 0.100000, 35 is 0.9253\n",
      "The accuracy score for 0.100000, 40 is 0.9332\n",
      "The accuracy score for 0.100000, 45 is 0.9304\n",
      "The accuracy score for 0.100000, 50 is 0.9245\n",
      "The accuracy score for 0.500000, 5 is 0.9226\n",
      "The accuracy score for 0.500000, 10 is 0.9277\n",
      "The accuracy score for 0.500000, 15 is 0.9277\n",
      "The accuracy score for 0.500000, 20 is 0.9288\n",
      "The accuracy score for 0.500000, 25 is 0.9228\n",
      "The accuracy score for 0.500000, 30 is 0.9264\n",
      "The accuracy score for 0.500000, 35 is 0.9261\n",
      "The accuracy score for 0.500000, 40 is 0.9174\n",
      "The accuracy score for 0.500000, 45 is 0.9255\n",
      "The accuracy score for 0.500000, 50 is 0.9245\n",
      "The accuracy score for 1.000000, 5 is 0.9212\n",
      "The accuracy score for 1.000000, 10 is 0.9239\n",
      "The accuracy score for 1.000000, 15 is 0.9223\n",
      "The accuracy score for 1.000000, 20 is 0.9168\n",
      "The accuracy score for 1.000000, 25 is 0.9245\n",
      "The accuracy score for 1.000000, 30 is 0.9223\n",
      "The accuracy score for 1.000000, 35 is 0.9226\n",
      "The accuracy score for 1.000000, 40 is 0.9179\n",
      "The accuracy score for 1.000000, 45 is 0.9242\n",
      "The accuracy score for 1.000000, 50 is 0.9185\n"
     ]
    }
   ],
   "source": [
    "## tune the parameters: 1. max_iter   2. learning rate\n",
    "# specified parameters, to simplify we choose to have one hidden layer with 5 neurons.\n",
    "pca_input_dim = pca_train_x.shape[1]\n",
    "output_dim = 2\n",
    "num_layer = 1\n",
    "num_nodes = 5\n",
    "\n",
    "\n",
    "# unspecifies parameters\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_iters = list(range(5, 55,5))\n",
    "pca_score_matrix = np.zeros((len(learning_rates), len(max_iters)))\n",
    "\n",
    "for l in range(len(learning_rates)):\n",
    "    for m in range(len(max_iters)):\n",
    "        learning_rate, max_iter = learning_rates[l], max_iters[m]\n",
    "        accu = cv_nn(nn_pca_X, nn_pca_Y, pca_input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "        print('The accuracy score for %f, %d is %.4f' %(learning_rate, max_iter, accu))\n",
    "        pca_score_matrix[l, m] = accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c0fccf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotHeatMap(pca_score_matrix, y_title='learning rate', x_title='max iter',\\\n",
    "            title='Accuracy Score', y_ticks=learning_rates, x_ticks=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pca_score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 3679: 0.307076\n",
      "Loss after iteration 3679: 0.257007\n",
      "Loss after iteration 3679: 0.238259\n",
      "Loss after iteration 3679: 0.226222\n",
      "Loss after iteration 3679: 0.218667\n",
      "Loss after iteration 3679: 0.215279\n",
      "Loss after iteration 3679: 0.212506\n",
      "Loss after iteration 3679: 0.210777\n",
      "Loss after iteration 3679: 0.208499\n",
      "Loss after iteration 3679: 0.207035\n",
      "Loss after iteration 3679: 0.205912\n",
      "Loss after iteration 3679: 0.205105\n",
      "Loss after iteration 3679: 0.203139\n",
      "Loss after iteration 3679: 0.202159\n",
      "Loss after iteration 3679: 0.202525\n",
      "Loss after iteration 3679: 0.200162\n",
      "Loss after iteration 3679: 0.199067\n",
      "Loss after iteration 3679: 0.197952\n",
      "Loss after iteration 3679: 0.198262\n",
      "Loss after iteration 3679: 0.196253\n",
      "Loss after iteration 3679: 0.195053\n",
      "Loss after iteration 3679: 0.193841\n",
      "Loss after iteration 3679: 0.194410\n",
      "Loss after iteration 3679: 0.192610\n",
      "Loss after iteration 3679: 0.191692\n",
      "Loss after iteration 3679: 0.191209\n",
      "Loss after iteration 3679: 0.190269\n",
      "Loss after iteration 3679: 0.190614\n",
      "Loss after iteration 3679: 0.189108\n",
      "Loss after iteration 3679: 0.188512\n",
      "Loss after iteration 3679: 0.188486\n",
      "Loss after iteration 3679: 0.187443\n",
      "Loss after iteration 3679: 0.186965\n",
      "Loss after iteration 3679: 0.186534\n",
      "Loss after iteration 3679: 0.185985\n",
      "Loss after iteration 3679: 0.186082\n",
      "Loss after iteration 3679: 0.185697\n",
      "Loss after iteration 3679: 0.185747\n",
      "Loss after iteration 3679: 0.185424\n",
      "Loss after iteration 3679: 0.184898\n",
      "         score\n",
      "ACCU  0.919653\n",
      "TPR   0.931459\n",
      "PPV   0.938053\n",
      "TNR   0.900568\n",
      "F1    0.934744\n",
      "          spam  not spam\n",
      "spam       530        39\n",
      "not spam    35       317\n"
     ]
    }
   ],
   "source": [
    "## find the optimum learning_rate and max_iter, train the training data and test the test_data\n",
    "learning_rate, max_iter = 0.1, 40\n",
    "NN = Neural_Network(pca_input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "NN.train(pca_train_x, pca_train_y, print_loss=True)\n",
    "pred = NN.predict(pca_test_x)\n",
    "# metrics \n",
    "metric, confusion = confusion_matrix(pred, pca_test_y)\n",
    "print(metric)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
