{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('spambase.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get the dummpy variable of labels\n",
    "df = pd.get_dummies(df, columns=[57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check the missing values, there are no missing values in every variable.\n",
    "sum(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_x  = df.iloc[:,:-2]\n",
    "df_y = df.iloc[:, [-1,-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.342396</td>\n",
       "      <td>0.330849</td>\n",
       "      <td>0.712781</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>-0.350228</td>\n",
       "      <td>-0.291762</td>\n",
       "      <td>-0.262533</td>\n",
       "      <td>-0.323267</td>\n",
       "      <td>-0.371324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.514251</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>0.623939</td>\n",
       "      <td>-0.308321</td>\n",
       "      <td>-0.103037</td>\n",
       "      <td>-0.045242</td>\n",
       "      <td>0.045293</td>\n",
       "      <td>-0.008723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345322</td>\n",
       "      <td>0.051904</td>\n",
       "      <td>0.435082</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>-0.256089</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.244717</td>\n",
       "      <td>-0.088001</td>\n",
       "      <td>-0.323267</td>\n",
       "      <td>1.086593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.026004</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>0.126189</td>\n",
       "      <td>0.423737</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.250536</td>\n",
       "      <td>1.228191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.145906</td>\n",
       "      <td>-0.165054</td>\n",
       "      <td>0.851631</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>1.364698</td>\n",
       "      <td>0.343648</td>\n",
       "      <td>0.193623</td>\n",
       "      <td>0.036666</td>\n",
       "      <td>1.973802</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.117364</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.440005</td>\n",
       "      <td>-0.079746</td>\n",
       "      <td>0.145905</td>\n",
       "      <td>2.220865</td>\n",
       "      <td>3.258378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.342396</td>\n",
       "      <td>-0.165054</td>\n",
       "      <td>-0.556700</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>0.472521</td>\n",
       "      <td>-0.350228</td>\n",
       "      <td>0.500183</td>\n",
       "      <td>1.308259</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.605791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.007510</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>-0.161917</td>\n",
       "      <td>-0.308321</td>\n",
       "      <td>-0.103037</td>\n",
       "      <td>-0.052144</td>\n",
       "      <td>-0.062459</td>\n",
       "      <td>-0.152205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.342396</td>\n",
       "      <td>-0.165054</td>\n",
       "      <td>-0.556700</td>\n",
       "      <td>-0.046894</td>\n",
       "      <td>0.472521</td>\n",
       "      <td>-0.350228</td>\n",
       "      <td>0.500183</td>\n",
       "      <td>1.308259</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.605791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111534</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.014908</td>\n",
       "      <td>-0.155181</td>\n",
       "      <td>-0.164369</td>\n",
       "      <td>-0.308321</td>\n",
       "      <td>-0.103037</td>\n",
       "      <td>-0.052144</td>\n",
       "      <td>-0.062459</td>\n",
       "      <td>-0.152205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.342396  0.330849  0.712781 -0.046894  0.011563 -0.350228 -0.291762   \n",
       "1  0.345322  0.051904  0.435082 -0.046894 -0.256089  0.672326  0.244717   \n",
       "2 -0.145906 -0.165054  0.851631 -0.046894  1.364698  0.343648  0.193623   \n",
       "3 -0.342396 -0.165054 -0.556700 -0.046894  0.472521 -0.350228  0.500183   \n",
       "4 -0.342396 -0.165054 -0.556700 -0.046894  0.472521 -0.350228  0.500183   \n",
       "\n",
       "         7         8         9     ...           47        48        49  \\\n",
       "0 -0.262533 -0.323267 -0.371324    ...    -0.111534 -0.158436 -0.514251   \n",
       "1 -0.088001 -0.323267  1.086593    ...    -0.111534 -0.158436 -0.026004   \n",
       "2  0.036666  1.973802  0.016420    ...    -0.111534 -0.117364  0.014683   \n",
       "3  1.308259  0.789376  0.605791    ...    -0.111534 -0.158436 -0.007510   \n",
       "4  1.308259  0.789376  0.605791    ...    -0.111534 -0.158436 -0.014908   \n",
       "\n",
       "         50        51        52        53        54        55        56  \n",
       "0 -0.155181  0.623939 -0.308321 -0.103037 -0.045242  0.045293 -0.008723  \n",
       "1 -0.155181  0.126189  0.423737  0.008762 -0.002443  0.250536  1.228191  \n",
       "2 -0.155181  0.008495  0.440005 -0.079746  0.145905  2.220865  3.258378  \n",
       "3 -0.155181 -0.161917 -0.308321 -0.103037 -0.052144 -0.062459 -0.152205  \n",
       "4 -0.155181 -0.164369 -0.308321 -0.103037 -0.052144 -0.062459 -0.152205  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardlize data\n",
    "df_x_std = (df_x-df_x.mean()) / df_x.std()\n",
    "df_x_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes):\n",
    "        # parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.num_layer = num_layer\n",
    "        self.num_nodes = num_nodes\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # initialize the weight matrix \n",
    "        self.W = [np.random.randn(self.input_dim, self.num_nodes)]  # input to hidden layer\n",
    "        self.B = [np.zeros((1, self.num_nodes))]\n",
    "        for _ in range(1, self.num_layer):\n",
    "            self.W.append(np.random.randn(self.num_nodes,self.num_nodes))   # hidden layer to hidden layer\n",
    "            self.B.append(np.zeros((1, self.num_nodes)))\n",
    "        self.W.append(np.random.randn(self.num_nodes,self.output_dim))  # hiddern layer to output \n",
    "        self.B.append(np.zeros((1, self.output_dim)))      \n",
    "    \n",
    "    # activate method: sigmoid\n",
    "    def sigmoid(self, s):\n",
    "        return 1.0/(1 + np.exp(-s))\n",
    "    def sigmod_prime(self, s):\n",
    "        return s * (1-s)\n",
    "    \n",
    "    ## forward\n",
    "    def forward(self, X):\n",
    "        # input layer to output layer forward\n",
    "        self.Z = [X]\n",
    "        for i in range(0, len(self.W)):\n",
    "            z = np.dot(self.Z[i], self.W[i]) + self.B[i]\n",
    "            sz = self.sigmoid(z)\n",
    "            self.Z.append(sz)\n",
    "        # return output\n",
    "        return self.Z[-1]\n",
    "    \n",
    "    # backward\n",
    "    def backward(self,Y, output):\n",
    "        self.deltaZ = [(output-Y) * self.sigmod_prime(output)]\n",
    "        self.deltaW = []\n",
    "        self.deltaB = []\n",
    "        for i in range(len(self.W)-1,-1,-1):\n",
    "            deltaW = np.dot(self.Z[i].T, self.deltaZ[-1])\n",
    "            self.deltaW = [deltaW] + self.deltaW\n",
    "            deltaB = np.sum(self.deltaZ[-1], axis=0)\n",
    "            self.deltaB = [deltaB] + self.deltaB\n",
    "            \n",
    "            # add backward deltaZ in self.Z\n",
    "            dz = np.multiply((self.deltaZ[-1] @ self.W[i].T), self.sigmod_prime(self.Z[i]))\n",
    "            self.deltaZ.append(dz)\n",
    "        \n",
    "        # update the parameters\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] += -1*self.learning_rate*self.deltaW[i]\n",
    "            self.B[i] += -1*self.learning_rate*self.deltaB[i]\n",
    "    \n",
    "    # train the data \n",
    "    def train(self, train_x, train_y, print_loss=False):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            output  = self.forward(train_x)\n",
    "            self.backward(train_y, output)\n",
    "            \n",
    "            # Forward propogation to calculate the predictions\n",
    "            if print_loss and i % 50 == 0:\n",
    "                obs = self.train_x.shape[0]\n",
    "                loss = - np.sum(train_y * np.log(output))\n",
    "                print(\"Loss after iteration %i: %f\" %(i, np.sum(loss) / obs))\n",
    "                \n",
    "    def predict(self, val_x):\n",
    "        # Do forward pass\n",
    "        pred = self.forward(val_x)\n",
    "        #get y_hat\n",
    "        y_hat = np.argmax(pred, axis=1)\n",
    "        return y_hat\n",
    "    \n",
    "    def accuracy_score(self,pred,y):\n",
    "        # Get total number of examples\n",
    "        m = y.shape[0]\n",
    "        y_true = y.argmax(axis=1)\n",
    "        # Calculate the number of wrong examples\n",
    "        error = np.sum(np.abs(pred-y_true)) \n",
    "        # Calculate accuracy\n",
    "        return (m-error) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## random split data, training dataset (80%) and test dataset (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "## data\n",
    "X = np.array(df_x_std)\n",
    "Y = np.array(df_y)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## run k-folder cross validation function \n",
    "def cv_train_test_split(X, Y, kfolder= 10):\n",
    "    X_split, Y_split = [], []\n",
    "    index = list(range(len(Y)))\n",
    "    random.shuffle(index)   ## shuffle the index to random select\n",
    "    fold_size = int(len(X) / kfolder) + (len(X)%kfolder > 0)\n",
    "    for i in range(kfolder):\n",
    "        X_fold = X[index[i*fold_size: (i+1)*fold_size]]\n",
    "        Y_fold = Y[index[i*fold_size: (i+1)*fold_size]]\n",
    "        X_split.append(X_fold)\n",
    "        Y_split.append(Y_fold)\n",
    "    return X_split, Y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data split\n",
    "nn_X, nn_Y = cv_train_test_split(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## run the cross validation function \n",
    "def cv_nn(X, Y, input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes):\n",
    "    scores = []\n",
    "    for i in range(10):\n",
    "        x_train, x_test = np.concatenate(X[:i] + X[i+1:], axis = 0), X[i]\n",
    "        y_train, y_test = np.concatenate(Y[:i] + Y[i+1:], axis = 0), Y[i]\n",
    "        NN = Neural_Network(input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "        NN.train(x_train, y_train)\n",
    "        pred = NN.predict(x_test)\n",
    "        scores.append(NN.accuracy_score(pred, y_test))\n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for 0.000100, 50 is 0.5875\n",
      "The accuracy score for 0.000100, 100 is 0.6897\n",
      "The accuracy score for 0.000100, 150 is 0.7147\n",
      "The accuracy score for 0.000100, 200 is 0.7087\n",
      "The accuracy score for 0.000100, 250 is 0.7486\n",
      "The accuracy score for 0.000100, 300 is 0.7861\n",
      "The accuracy score for 0.000100, 350 is 0.8280\n",
      "The accuracy score for 0.000100, 400 is 0.7921\n",
      "The accuracy score for 0.000100, 450 is 0.8155\n",
      "The accuracy score for 0.000100, 500 is 0.8296\n",
      "The accuracy score for 0.000500, 50 is 0.7682\n",
      "The accuracy score for 0.000500, 100 is 0.8454\n",
      "The accuracy score for 0.000500, 150 is 0.8690\n",
      "The accuracy score for 0.000500, 200 is 0.8864\n",
      "The accuracy score for 0.000500, 250 is 0.8913\n",
      "The accuracy score for 0.000500, 300 is 0.9016\n",
      "The accuracy score for 0.000500, 350 is 0.9109\n",
      "The accuracy score for 0.000500, 400 is 0.9204\n",
      "The accuracy score for 0.000500, 450 is 0.9155\n",
      "The accuracy score for 0.000500, 500 is 0.9201\n",
      "The accuracy score for 0.001000, 50 is 0.8372\n",
      "The accuracy score for 0.001000, 100 is 0.8976\n",
      "The accuracy score for 0.001000, 150 is 0.8984\n",
      "The accuracy score for 0.001000, 200 is 0.9166\n",
      "The accuracy score for 0.001000, 250 is 0.9196\n",
      "The accuracy score for 0.001000, 300 is 0.9196\n",
      "The accuracy score for 0.001000, 350 is 0.9177\n",
      "The accuracy score for 0.001000, 400 is 0.9255\n",
      "The accuracy score for 0.001000, 450 is 0.9228\n",
      "The accuracy score for 0.001000, 500 is 0.9250\n",
      "The accuracy score for 0.005000, 50 is 0.9139\n",
      "The accuracy score for 0.005000, 100 is 0.9255\n",
      "The accuracy score for 0.005000, 150 is 0.9342\n",
      "The accuracy score for 0.005000, 200 is 0.9307\n",
      "The accuracy score for 0.005000, 250 is 0.9332\n",
      "The accuracy score for 0.005000, 300 is 0.9321\n",
      "The accuracy score for 0.005000, 350 is 0.9329\n",
      "The accuracy score for 0.005000, 400 is 0.9370\n",
      "The accuracy score for 0.005000, 450 is 0.9380\n",
      "The accuracy score for 0.005000, 500 is 0.9359\n",
      "The accuracy score for 0.010000, 50 is 0.9272\n",
      "The accuracy score for 0.010000, 100 is 0.9321\n",
      "The accuracy score for 0.010000, 150 is 0.9293\n",
      "The accuracy score for 0.010000, 200 is 0.9332\n",
      "The accuracy score for 0.010000, 250 is 0.9345\n",
      "The accuracy score for 0.010000, 300 is 0.9321\n",
      "The accuracy score for 0.010000, 350 is 0.9361\n",
      "The accuracy score for 0.010000, 400 is 0.9370\n",
      "The accuracy score for 0.010000, 450 is 0.9378\n",
      "The accuracy score for 0.010000, 500 is 0.9356\n",
      "The accuracy score for 0.050000, 50 is 0.6927\n",
      "The accuracy score for 0.050000, 100 is 0.7688\n",
      "The accuracy score for 0.050000, 150 is 0.7628\n",
      "The accuracy score for 0.050000, 200 is 0.8269\n",
      "The accuracy score for 0.050000, 250 is 0.7679\n",
      "The accuracy score for 0.050000, 300 is 0.7304\n",
      "The accuracy score for 0.050000, 350 is 0.7465\n",
      "The accuracy score for 0.050000, 400 is 0.8948\n",
      "The accuracy score for 0.050000, 450 is 0.8269\n",
      "The accuracy score for 0.050000, 500 is 0.8364\n",
      "The accuracy score for 0.100000, 50 is 0.6128\n",
      "The accuracy score for 0.100000, 100 is 0.6410\n",
      "The accuracy score for 0.100000, 150 is 0.6190\n",
      "The accuracy score for 0.100000, 200 is 0.7204\n",
      "The accuracy score for 0.100000, 250 is 0.7258\n",
      "The accuracy score for 0.100000, 300 is 0.7440\n",
      "The accuracy score for 0.100000, 350 is 0.7054\n",
      "The accuracy score for 0.100000, 400 is 0.7185\n",
      "The accuracy score for 0.100000, 450 is 0.6726\n",
      "The accuracy score for 0.100000, 500 is 0.5989\n",
      "The accuracy score for 0.500000, 50 is 0.5386\n",
      "The accuracy score for 0.500000, 100 is 0.5035\n",
      "The accuracy score for 0.500000, 150 is 0.4845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for 0.500000, 200 is 0.5685\n",
      "The accuracy score for 0.500000, 250 is 0.5410\n",
      "The accuracy score for 0.500000, 300 is 0.4976\n",
      "The accuracy score for 0.500000, 350 is 0.5655\n",
      "The accuracy score for 0.500000, 400 is 0.6005\n",
      "The accuracy score for 0.500000, 450 is 0.6688\n",
      "The accuracy score for 0.500000, 500 is 0.6060\n",
      "The accuracy score for 1.000000, 50 is 0.4924\n",
      "The accuracy score for 1.000000, 100 is 0.5429\n",
      "The accuracy score for 1.000000, 150 is 0.6215\n",
      "The accuracy score for 1.000000, 200 is 0.5815\n",
      "The accuracy score for 1.000000, 250 is 0.6190\n",
      "The accuracy score for 1.000000, 300 is 0.5989\n",
      "The accuracy score for 1.000000, 350 is 0.5821\n",
      "The accuracy score for 1.000000, 400 is 0.5318\n",
      "The accuracy score for 1.000000, 450 is 0.5024\n",
      "The accuracy score for 1.000000, 500 is 0.5378\n"
     ]
    }
   ],
   "source": [
    "## tune the parameters: 1. max_iter   2. learning rate\n",
    "# specified parameters, to simplify we choose to have one hidden layer with 5 neurons.\n",
    "input_dim = train_x.shape[1]\n",
    "output_dim = 2\n",
    "num_layer = 1\n",
    "num_nodes = 5\n",
    "\n",
    "\n",
    "# unspecifies parameters\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_iters = list(range(50, 550,50))\n",
    "score_matrix = np.zeros((len(learning_rates), len(max_iters)))\n",
    "\n",
    "for l in range(len(learning_rates)):\n",
    "    for m in range(len(max_iters)):\n",
    "        learning_rate, max_iter = learning_rates[l], max_iters[m]\n",
    "        accu = cv_nn(nn_X, nn_Y, input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "        print('The accuracy score for %f, %d is %.4f' %(learning_rate, max_iter, accu))\n",
    "        score_matrix[l, m] = accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## heatmap\n",
    "def plotHeatMap(data, x_title='X Axis', y_title='Y Axis', title='', x_ticks=[], y_ticks=[]):\n",
    "    \n",
    "    #Plot it out\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    heatmap = ax.pcolor(data, edgecolors='k', alpha=0.8)\n",
    "    \n",
    "    fig.colorbar(heatmap, ax=ax)\n",
    "    \n",
    "    # turn off the frame\n",
    "    #ax.set_frame_on(False)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(len(x_ticks)) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(len(y_ticks)) + 0.5, minor=False)\n",
    "    \n",
    "    ax.set_xticklabels([str(i) for i in x_ticks], minor=False)\n",
    "    ax.set_yticklabels([str(y) for y in y_ticks], minor=False)\n",
    "    \n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cab0dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotHeatMap(score_matrix, y_title='learning rate', x_title='max iter',\\\n",
    "            title='Accuracy Score', y_ticks=learning_rates, x_ticks=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(pred, y):\n",
    "    m = y.shape[0]\n",
    "    y_true = y.argmax(axis=1)\n",
    "    \n",
    "    FP = sum((y_true==0) & (pred==1))\n",
    "    FN = sum((y_true==1) & (pred==0))\n",
    "    TP = sum((y_true==1) & (pred==1))\n",
    "    TN = sum((y_true==0) & (pred==0))\n",
    "    \n",
    "    ## print performance metrics\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    TPR = TP / (TP + FN)\n",
    "    PPV = TP / (TP + FP)\n",
    "    TNR = TN / (TN + FP)\n",
    "    F1_score = 2*PPV*TPR / (PPV+TPR)\n",
    "    metric = pd.DataFrame([accuracy, TPR, PPV, TNR, F1_score], ['ACCU', 'TPR', 'PPV', 'TNR', 'F1'], ['score'])\n",
    "    \n",
    "    ### print confusion matrix\n",
    "    conf_matrix = np.array([[TP, FN], [FP, TN]])\n",
    "    conf = pd.DataFrame(conf_matrix, ['spam', 'not spam'], ['spam', 'not spam'])\n",
    "    return metric, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.673941\n",
      "Loss after iteration 50: 0.230135\n",
      "Loss after iteration 100: 0.188224\n",
      "Loss after iteration 150: 0.170416\n",
      "Loss after iteration 200: 0.161796\n",
      "Loss after iteration 250: 0.156427\n",
      "Loss after iteration 300: 0.149862\n",
      "Loss after iteration 350: 0.146285\n",
      "Loss after iteration 400: 0.143431\n",
      "         score\n",
      "ACCU  0.931596\n",
      "TPR   0.945205\n",
      "PPV   0.946827\n",
      "TNR   0.908012\n",
      "F1    0.946015\n",
      "          spam  not spam\n",
      "spam       552        32\n",
      "not spam    31       306\n"
     ]
    }
   ],
   "source": [
    "## find the optimum learning_rate and max_iter, train the training data and test the test_data\n",
    "learning_rate, max_iter = 0.005, 450\n",
    "NN = Neural_Network(input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "NN.train(train_x, train_y, print_loss=True)\n",
    "pred = NN.predict(test_x)\n",
    "# metrics \n",
    "metric, confusion = confusion_matrix(pred, test_y)\n",
    "print(metric)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNs with Reduced Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11564794 0.17297118 0.2081142  0.23641524 0.26354169 0.28920029\n",
      " 0.314007   0.33812839 0.36085269 0.38325521 0.4046048  0.42443115\n",
      " 0.4439361  0.46314837 0.48221912 0.50087647 0.51927371 0.53722774\n",
      " 0.55499377 0.57258701 0.59005918 0.60722171 0.62414334 0.64065816\n",
      " 0.65708953 0.67330598 0.68936487 0.70523575 0.72055892 0.73575189\n",
      " 0.75042723 0.7649313  0.77892553 0.79263965 0.80626455 0.81951942\n",
      " 0.83239801 0.84508326 0.85744449 0.86954722 0.88138239 0.8930665\n",
      " 0.90392819 0.91460277 0.92481097 0.93493523 0.94413616 0.95271015\n",
      " 0.96061065 0.96778785 0.97437741 0.98079342 0.98666807 0.99202564\n",
      " 0.99659442 0.99993237 1.        ]\n"
     ]
    }
   ],
   "source": [
    "## run pca analysis, get the principle components, the first 43 components (90% of variance)\n",
    "u, s, vh = np.linalg.svd(df_x_std)\n",
    "\n",
    "s2 = np.cumsum(s**2) / np.sum(s**2)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_x = df_x_std @ vh.T[:, :43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## data\n",
    "X_PC = np.array(pc_x)\n",
    "Y_PC = np.array(df_y)\n",
    "\n",
    "pca_train_x, pca_test_x, pca_train_y, pca_test_y = train_test_split(X_PC, Y_PC, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data split\n",
    "nn_pca_X, nn_pca_Y = cv_train_test_split(pca_train_x, pca_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for 0.000100, 50 is 0.6087\n",
      "The accuracy score for 0.000100, 100 is 0.6889\n",
      "The accuracy score for 0.000100, 150 is 0.7168\n",
      "The accuracy score for 0.000100, 200 is 0.7791\n",
      "The accuracy score for 0.000100, 250 is 0.7908\n",
      "The accuracy score for 0.000100, 300 is 0.7957\n",
      "The accuracy score for 0.000100, 350 is 0.8022\n",
      "The accuracy score for 0.000100, 400 is 0.8274\n",
      "The accuracy score for 0.000100, 450 is 0.8405\n",
      "The accuracy score for 0.000100, 500 is 0.8429\n",
      "The accuracy score for 0.000500, 50 is 0.7889\n",
      "The accuracy score for 0.000500, 100 is 0.8399\n",
      "The accuracy score for 0.000500, 150 is 0.8924\n",
      "The accuracy score for 0.000500, 200 is 0.8788\n",
      "The accuracy score for 0.000500, 250 is 0.8995\n",
      "The accuracy score for 0.000500, 300 is 0.9092\n",
      "The accuracy score for 0.000500, 350 is 0.9106\n",
      "The accuracy score for 0.000500, 400 is 0.9171\n",
      "The accuracy score for 0.000500, 450 is 0.9185\n",
      "The accuracy score for 0.000500, 500 is 0.9231\n",
      "The accuracy score for 0.001000, 50 is 0.8318\n",
      "The accuracy score for 0.001000, 100 is 0.8712\n",
      "The accuracy score for 0.001000, 150 is 0.9071\n",
      "The accuracy score for 0.001000, 200 is 0.9152\n",
      "The accuracy score for 0.001000, 250 is 0.9158\n",
      "The accuracy score for 0.001000, 300 is 0.9220\n",
      "The accuracy score for 0.001000, 350 is 0.9228\n",
      "The accuracy score for 0.001000, 400 is 0.9245\n",
      "The accuracy score for 0.001000, 450 is 0.9272\n",
      "The accuracy score for 0.001000, 500 is 0.9304\n",
      "The accuracy score for 0.005000, 50 is 0.9204\n",
      "The accuracy score for 0.005000, 100 is 0.9283\n",
      "The accuracy score for 0.005000, 150 is 0.9318\n",
      "The accuracy score for 0.005000, 200 is 0.9285\n",
      "The accuracy score for 0.005000, 250 is 0.9326\n",
      "The accuracy score for 0.005000, 300 is 0.9318\n",
      "The accuracy score for 0.005000, 350 is 0.9356\n",
      "The accuracy score for 0.005000, 400 is 0.9329\n",
      "The accuracy score for 0.005000, 450 is 0.9342\n",
      "The accuracy score for 0.005000, 500 is 0.9348\n",
      "The accuracy score for 0.010000, 50 is 0.9269\n",
      "The accuracy score for 0.010000, 100 is 0.9307\n",
      "The accuracy score for 0.010000, 150 is 0.9321\n",
      "The accuracy score for 0.010000, 200 is 0.9304\n",
      "The accuracy score for 0.010000, 250 is 0.9340\n",
      "The accuracy score for 0.010000, 300 is 0.9364\n",
      "The accuracy score for 0.010000, 350 is 0.9332\n",
      "The accuracy score for 0.010000, 400 is 0.9364\n",
      "The accuracy score for 0.010000, 450 is 0.9329\n",
      "The accuracy score for 0.010000, 500 is 0.9353\n",
      "The accuracy score for 0.050000, 50 is 0.8024\n",
      "The accuracy score for 0.050000, 100 is 0.7079\n",
      "The accuracy score for 0.050000, 150 is 0.7967\n",
      "The accuracy score for 0.050000, 200 is 0.7370\n",
      "The accuracy score for 0.050000, 250 is 0.7557\n",
      "The accuracy score for 0.050000, 300 is 0.8432\n",
      "The accuracy score for 0.050000, 350 is 0.8332\n",
      "The accuracy score for 0.050000, 400 is 0.8948\n",
      "The accuracy score for 0.050000, 450 is 0.7734\n",
      "The accuracy score for 0.050000, 500 is 0.7796\n",
      "The accuracy score for 0.100000, 50 is 0.7307\n",
      "The accuracy score for 0.100000, 100 is 0.6535\n",
      "The accuracy score for 0.100000, 150 is 0.6541\n",
      "The accuracy score for 0.100000, 200 is 0.6701\n",
      "The accuracy score for 0.100000, 250 is 0.6410\n",
      "The accuracy score for 0.100000, 300 is 0.7038\n",
      "The accuracy score for 0.100000, 350 is 0.5818\n",
      "The accuracy score for 0.100000, 400 is 0.6432\n",
      "The accuracy score for 0.100000, 450 is 0.6516\n",
      "The accuracy score for 0.100000, 500 is 0.7152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for 0.500000, 50 is 0.5905\n",
      "The accuracy score for 0.500000, 100 is 0.5856\n",
      "The accuracy score for 0.500000, 150 is 0.6057\n",
      "The accuracy score for 0.500000, 200 is 0.5905\n",
      "The accuracy score for 0.500000, 250 is 0.5410\n",
      "The accuracy score for 0.500000, 300 is 0.5530\n",
      "The accuracy score for 0.500000, 350 is 0.5861\n",
      "The accuracy score for 0.500000, 400 is 0.6788\n",
      "The accuracy score for 0.500000, 450 is 0.5799\n",
      "The accuracy score for 0.500000, 500 is 0.5207\n",
      "The accuracy score for 1.000000, 50 is 0.4935\n",
      "The accuracy score for 1.000000, 100 is 0.6114\n",
      "The accuracy score for 1.000000, 150 is 0.4905\n",
      "The accuracy score for 1.000000, 200 is 0.5111\n",
      "The accuracy score for 1.000000, 250 is 0.6228\n",
      "The accuracy score for 1.000000, 300 is 0.5356\n",
      "The accuracy score for 1.000000, 350 is 0.5992\n",
      "The accuracy score for 1.000000, 400 is 0.5783\n",
      "The accuracy score for 1.000000, 450 is 0.5851\n",
      "The accuracy score for 1.000000, 500 is 0.5802\n"
     ]
    }
   ],
   "source": [
    "## tune the parameters: 1. max_iter   2. learning rate\n",
    "# specified parameters, to simplify we choose to have one hidden layer with 5 neurons.\n",
    "pca_input_dim = pca_train_x.shape[1]\n",
    "output_dim = 2\n",
    "num_layer = 1\n",
    "num_nodes = 5\n",
    "\n",
    "\n",
    "# unspecifies parameters\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_iters = list(range(50, 550,50))\n",
    "pca_score_matrix = np.zeros((len(learning_rates), len(max_iters)))\n",
    "\n",
    "for l in range(len(learning_rates)):\n",
    "    for m in range(len(max_iters)):\n",
    "        learning_rate, max_iter = learning_rates[l], max_iters[m]\n",
    "        accu = cv_nn(nn_pca_X, nn_pca_Y, pca_input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "        print('The accuracy score for %f, %d is %.4f' %(learning_rate, max_iter, accu))\n",
    "        pca_score_matrix[l, m] = accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x62c2b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotHeatMap(pca_score_matrix, y_title='learning rate', x_title='max iter',\\\n",
    "            title='Accuracy Score', y_ticks=learning_rates, x_ticks=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pca_score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.006612\n",
      "Loss after iteration 50: 0.200568\n",
      "Loss after iteration 100: 0.175663\n",
      "Loss after iteration 150: 0.169453\n",
      "Loss after iteration 200: 0.167049\n",
      "Loss after iteration 250: 0.165925\n",
      "         score\n",
      "ACCU  0.920738\n",
      "TPR   0.950092\n",
      "PPV   0.917857\n",
      "TNR   0.878947\n",
      "F1    0.933697\n",
      "          spam  not spam\n",
      "spam       514        27\n",
      "not spam    46       334\n"
     ]
    }
   ],
   "source": [
    "## find the optimum learning_rate and max_iter, train the training data and test the test_data\n",
    "learning_rate, max_iter = 0.01, 300\n",
    "NN = Neural_Network(pca_input_dim, output_dim, learning_rate, max_iter, num_layer, num_nodes)\n",
    "NN.train(pca_train_x, pca_train_y, print_loss=True)\n",
    "pred = NN.predict(pca_test_x)\n",
    "# metrics \n",
    "metric, confusion = confusion_matrix(pred, pca_test_y)\n",
    "print(metric)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
